{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading cryptography-45.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\preksha\\appdata\\roaming\\python\\python313\\site-packages (from pytesseract) (25.0)\n",
      "Collecting cffi>=1.14 (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 2.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.3/5.6 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.6 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.1/5.6 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.6/5.6 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 3.7/5.6 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.8/2.7 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.5 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.0/3.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
      "Downloading cryptography-45.0.3-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.3/3.4 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.9/3.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, Pillow, charset-normalizer, pytesseract, pdf2image, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed Pillow-11.2.1 cffi-1.17.1 charset-normalizer-3.4.2 cryptography-45.0.3 pdf2image-1.17.0 pdfminer.six-20250327 pdfplumber-0.11.6 pycparser-2.22 pypdfium2-4.30.1 pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber pytesseract pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try direct text extraction\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Direct text extraction failed: {e}\")\n",
    "\n",
    "    # Fallback to OCR for image-based PDFs\n",
    "    print(\"Falling back to OCR for image-based PDF.\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for image in images:\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed: {e}\")\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct text extraction failed: [Errno 2] No such file or directory: 'Resume.pdf'\n",
      "Falling back to OCR for image-based PDF.\n",
      "OCR failed: Unable to get page count. Is poppler installed and in PATH?\n",
      "\n",
      "Extracted Text from PDF:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Resume.pdf\"\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"\\nExtracted Text from PDF:\")\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Google GenerativeAI Api Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google.generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google.generativeai)\n",
      "  Downloading google_api_python_client-2.170.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google.generativeai)\n",
      "  Downloading google_auth-2.40.2-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting protobuf (from google.generativeai)\n",
      "  Downloading protobuf-6.31.0-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pydantic (from google.generativeai)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting tqdm (from google.generativeai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions (from google.generativeai)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Downloading google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google.generativeai)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google.generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting requests<3.0.0,>=2.18.0 (from google-api-core->google.generativeai)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google.generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google.generativeai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->google.generativeai)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->google.generativeai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\preksha\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio-1.71.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\preksha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.4.2)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.2-py2.py3-none-any.whl (216 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading google_api_python_client-2.170.0-py3-none-any.whl (13.5 MB)\n",
      "   ---------------------------------------- 0.0/13.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.6/13.5 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.1/13.5 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.5 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.5/13.5 MB 20.0 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 22.3 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading grpcio-1.71.0-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.6/4.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, uritemplate, typing-extensions, tqdm, python-dotenv, pyparsing, pyasn1, protobuf, idna, grpcio, certifi, cachetools, annotated-types, typing-inspection, rsa, requests, pydantic-core, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, pydantic, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google.generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.2 certifi-2025.4.26 google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.170.0 google-auth-2.40.2 google-auth-httplib2-0.2.0 google.generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 idna-3.10 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.5 pydantic-core-2.33.2 pyparsing-3.2.3 python-dotenv-1.1.0 requests-2.32.3 rsa-4.9.1 tqdm-4.67.1 typing-extensions-4.13.2 typing-inspection-0.4.1 uritemplate-4.1.1 urllib3-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install google.generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Manually set your API key as a string\n",
    "api_key = \"AIzaSyBF9QyoiZL49lKzjzpOJZXXmXkkeTdPyHE\"  # Ensure it's inside quotes\n",
    "\n",
    "# Configure generative AI model\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Test the model with a sample query\n",
    "try:\n",
    "    response = model.generate_content(\"What is the capital of India?\")\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating content: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Candidate Assessment: AI Engineer\n",
      "\n",
      "**1. Alignment with the Role:**\n",
      "\n",
      "The candidate's resume indicates a basic alignment with the AI Engineer job description.  The mention of \"Python, Machine Learning, and Cloud Computing\" suggests foundational skills relevant to the role. However, the lack of explicit mention of NLP (Natural Language Processing) is a significant omission, given its prominence in the job description.  The resume is too brief to confidently assess the depth of their expertise in ML and cloud platforms.  More detail is needed.\n",
      "\n",
      "**2. Current Skills and Suggested Improvements:**\n",
      "\n",
      "**Current Skills:**\n",
      "\n",
      "* **Python:**  Level of proficiency unknown, needs further clarification.  Specific libraries used (e.g., TensorFlow, PyTorch, scikit-learn) should be listed.\n",
      "* **Machine Learning:**  Basic understanding assumed, but the breadth and depth are unclear.  Specific ML algorithms (e.g., regression, classification, clustering) and applications should be specified.\n",
      "* **Cloud Computing:**  Which platform(s)? (AWS, Azure, GCP).  Specific services used (e.g., EC2, S3, Kubernetes) are crucial for evaluating expertise.\n",
      "\n",
      "**Suggested Improvements:**\n",
      "\n",
      "* **Quantify achievements:** Instead of simply listing skills, quantify accomplishments.  For example, \"Improved model accuracy by 15% using XGBoost\" or \"Reduced model training time by 20% by optimizing hyperparameters.\"\n",
      "* **Specify projects:** Detail projects undertaken, including technologies used, challenges overcome, and results achieved.  GitHub repositories, if available, should be linked.\n",
      "* **Add NLP skills:**  Given the job description, explicitly mentioning experience with NLP techniques (e.g., sentiment analysis, text classification, named entity recognition) and relevant libraries (e.g., NLTK, spaCy, Transformers) is crucial.\n",
      "* **Clarify cloud expertise:**  Specify the cloud platform(s) used and relevant services.\n",
      "* **Showcase soft skills:**  Highlight relevant soft skills like teamwork, communication, problem-solving, and ability to work independently.\n",
      "\n",
      "\n",
      "**3. Recommended Courses for Skill Enhancement:**\n",
      "\n",
      "Depending on the gaps identified after a more detailed interview and portfolio review:\n",
      "\n",
      "* **NLP Specialization:** Coursera, edX, Udacity offer specialized courses covering various aspects of NLP.\n",
      "* **Advanced Machine Learning:**  Courses focusing on deep learning, model deployment, and MLOps are beneficial.  Platforms like fast.ai, DataCamp, and Udemy offer relevant options.\n",
      "* **Cloud Computing Certifications:** AWS Certified Machine Learning – Specialty, Azure AI Engineer Associate, or Google Cloud Certified Professional Data Engineer are valuable certifications depending on the chosen platform.\n",
      "* **Specific library deep dives:**  Targeted courses or tutorials on TensorFlow, PyTorch, or other relevant libraries can improve practical skills.\n",
      "\n",
      "\n",
      "**4. Strengths and Weaknesses (Based on Job Description):**\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* Foundational skills in Python and Machine Learning are present, suggesting a basic level of competence.\n",
      "* Experience with Cloud Computing is mentioned, although the specifics are unknown.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Lack of NLP expertise:** This is a major weakness, as NLP is explicitly required in the job description.\n",
      "* **Lack of detail:** The resume provides minimal information, making it difficult to assess the candidate's true capabilities.\n",
      "* **Unspecified Cloud Platform:** Without knowing the specific cloud platform(s) used, it’s hard to gauge the candidate's level of expertise in cloud-based AI development.\n",
      "* **No quantifiable achievements:**  The resume lacks examples demonstrating the impact of the candidate's work.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The candidate shows potential but needs to significantly strengthen their resume and demonstrably improve their NLP skills to be a competitive applicant.  A detailed interview and portfolio review are crucial for a thorough evaluation. The brevity of the resume is a significant hindrance; a more comprehensive presentation of skills and experience is strongly recommended.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Manually set the API key\n",
    "api_key = \"AIzaSyBF9QyoiZL49lKzjzpOJZXXmXkkeTdPyHE\"  # Replace with your valid API key\n",
    "\n",
    "# Configure Generative AI model\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# Resume analysis function\n",
    "def analyze_resume(resume_text, job_description=None):\n",
    "    if not resume_text or resume_text.strip() == \"\":\n",
    "        return {\"error\": \"Resume text is required for analysis.\"}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an experienced HR with technical expertise in various job roles:\n",
    "    - Data Science, Data Analyst, DevOps, Machine Learning Engineer, AI Engineer, Full Stack Web Developer, \n",
    "    - Big Data Engineer, Marketing Analyst, Human Resource Manager, Software Developer.\n",
    "\n",
    "    Your task:\n",
    "    1️⃣ Assess if the candidate's profile aligns with the role.\n",
    "    2️⃣ Identify their current skills and suggest improvements.\n",
    "    3️⃣ Recommend courses for skill enhancement.\n",
    "    4️⃣ Highlight strengths and weaknesses.\n",
    "\n",
    "    Resume:\n",
    "    {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "    if job_description:\n",
    "        prompt += f\"\"\"\n",
    "        Additionally, compare this resume to the following job description:\n",
    "\n",
    "        Job Description:\n",
    "        {job_description}\n",
    "\n",
    "        Highlight the strengths and weaknesses of the applicant based on job requirements.\n",
    "        \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error generating content: {e}\"}\n",
    "\n",
    "# Example test case\n",
    "resume_sample = \"Experienced AI Engineer skilled in Python, Machine Learning, and Cloud Computing.\"\n",
    "job_desc_sample = \"Looking for an AI Engineer with strong expertise in ML, NLP, and cloud platforms.\"\n",
    "output = analyze_resume(resume_sample, job_desc_sample)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
